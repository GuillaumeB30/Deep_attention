{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e7fcd268-afaa-4537-9a25-b31015ac3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b28d6ac-6587-4372-a7e6-30ccc193aa29",
   "metadata": {},
   "source": [
    "# Création de la classe Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "11ca8946-14c1-412b-8ceb-bbcc9b7891bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour le débuggage vérifie qu'il n'y a pas de nan dans les tenseurs\n",
    "def check_for_nans(tensor, tensor_name):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"NaN detected in {tensor_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f4c4f7-aeae-4eb8-9e6a-425adc80d048",
   "metadata": {},
   "source": [
    "Création des différentes couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "0b435002-af4e-4743-8939-31101cb2422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "0636a980-6485-4023-b485-732845a49f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(x):\n",
    "    len = x.size(0)\n",
    "    mask = torch.triu(torch.ones(len,len), diagonal = 1) * (-1e9) # Matrice triangulaire supérieur de valeur -inf\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "7f89e55d-5a9d-4186-98c1-3f4d042fc7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, max_length, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "1b1a857c-a787-4f5b-8408-093e8a636158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.attention = torch.nn.MultiheadAttention(d_model, n_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_output, wei = self.attention(x, x, x)\n",
    "        x = self.norm1(x + attention_output)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "2df9652d-bc17-4923-a110-72fb57893c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.attention1 = torch.nn.MultiheadAttention(d_model, n_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attention2 = torch.nn.MultiheadAttention(d_model, n_heads)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, enc_output, mask):\n",
    "        attention_output1, wei = self.attention1(x, x, x,attn_mask = mask)\n",
    "        x = self.norm1(x + attention_output1) \n",
    "        attention_output2, wei = self.attention2(x, enc_output, enc_output)\n",
    "        x = self.norm2(x + attention_output2)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm3(x + ffn_output)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "2f440a5e-05d8-48ad-97c3-8b42eeaa1e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, target_size, max_length, d_model, num_heads, d_ff, n_layers):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.enc_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.dec_embedding = nn.Embedding(target_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(max_length, d_model)\n",
    "        \n",
    "        self.encoder_layers = [Encoder(d_model, num_heads,d_ff) for i in range(n_layers)]\n",
    "        self.decoder_layers = [Decoder(d_model, num_heads, d_ff) for i in range(n_layers)]\n",
    "\n",
    "        self.linear = nn.Linear(d_model, target_size)\n",
    "\n",
    "    def check_for_nans(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if torch.isnan(param).any():\n",
    "                print(f\"NaN detected in parameter: {name}\")\n",
    "\n",
    "    def forward(self,inp,out):\n",
    "\n",
    "        check_for_nans(inp,\"inp\")\n",
    "        check_for_nans(out, \"out\")\n",
    "        \n",
    "        mask = create_mask(out)\n",
    "\n",
    "        check_for_nans(mask, \"mask\")\n",
    "        \n",
    "        out_embedded = self.positional_encoding(self.dec_embedding(out))\n",
    "        inp_embedded = self.positional_encoding(self.enc_embedding(inp))\n",
    "\n",
    "        check_for_nans(out_embedded, \"out_embedded\")\n",
    "        check_for_nans(inp_embedded, \"inp_embedded\")\n",
    "        \n",
    "        enc_output = inp_embedded\n",
    "        for encoder in self.encoder_layers:\n",
    "            enc_output = encoder(enc_output)\n",
    "\n",
    "        check_for_nans(enc_output, \"enc_output\")\n",
    "        \n",
    "        dec_output = out_embedded\n",
    "        for decoder in self.decoder_layers:\n",
    "            dec_output = decoder(dec_output, enc_output, mask)\n",
    "\n",
    "        check_for_nans(dec_output, \"dec_output\")\n",
    "        \n",
    "        output = self.linear(dec_output)\n",
    "\n",
    "        check_for_nans(output, \"output\")\n",
    "        return output   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bca86b-2ff1-4662-b9c4-8eb454e41c00",
   "metadata": {},
   "source": [
    "# Mise en forme des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "de786ef9-1eff-42d1-8194-4089c3bfb59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "60794702-5ac3-4811-b9af-87a218cb3249",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "18c64f66-c4ab-4f7b-b0c6-dab3315ece99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On importe le dataset\n",
    "x = []\n",
    "y = []\n",
    "with open(\"eng_-french.csv\",encoding = \"utf-8\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        y.append(row[0])\n",
    "        x.append(row[-1])\n",
    "    x.pop(0)\n",
    "    y.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "967b8ebd-00d2-416e-9912-8aa2e7a9bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On n'utilise qu'une partie du dataset\n",
    "x_red = x[:40000]\n",
    "y_red = y[:40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "a43238cc-b560-43c9-8b7e-54c966e14c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "9f7bdd50-a7f2-46e1-b458-61c837afead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_red, y_red, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "91a6d0a8-833c-4907-9df4-3829664415b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On tokenize les phrase\n",
    "x_train = [word_tokenize(word) for word in x_train]\n",
    "y_train = [word_tokenize(word) for word in y_train]\n",
    "x_test = [word_tokenize(word) for word in x_test]\n",
    "y_test = [word_tokenize(word) for word in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "f8082190-7af8-422d-b9e3-e9555a376944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On retire la ponctuation\n",
    "for i in range(len(x_train)):\n",
    "    for j in range(len(x_train[i])-1,-1,-1):\n",
    "        if x_train[i][j] in punctuation:\n",
    "            x_train[i].pop(j)\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    for j in range(len(y_train[i])-1,-1,-1):\n",
    "        if y_train[i][j] in punctuation:\n",
    "            y_train[i].pop(j)\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    for j in range(len(x_test[i])-1,-1,-1):\n",
    "        if x_test[i][j] in punctuation:\n",
    "            x_test[i].pop(j)\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    for j in range(len(y_test[i])-1,-1,-1):\n",
    "        if y_test[i][j] in punctuation:\n",
    "            y_test[i].pop(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "742b3078-e24b-4aa9-92e6-9151cf6b2a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On lemmise tous nos mots\n",
    "stemmer = PorterStemmer()\n",
    "for i in range(len(y_train)):\n",
    "    y_train[i] = [stemmer.stem(word) for word in y_train[i]]\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    x_train[i] = [stemmer.stem(word) for word in x_train[i]]\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    y_test[i] = [stemmer.stem(word) for word in y_test[i]]\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    x_test[i] = [stemmer.stem(word) for word in x_test[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "ab48ef8b-7698-40b7-872e-bbea32e9a233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8110\n",
      "3610\n"
     ]
    }
   ],
   "source": [
    "# On récupère la taille des 2 vocabulaires (francais/anglais)\n",
    "vocab = []\n",
    "target_vocab = []\n",
    "for sentence in x_train:\n",
    "    for word in sentence:\n",
    "        if word not in vocab:\n",
    "            vocab.append(word)\n",
    "\n",
    "for sentence in y_train:\n",
    "    for word in sentence:\n",
    "        if word not in target_vocab:\n",
    "            target_vocab.append(word)\n",
    "\n",
    "print(len(vocab))\n",
    "print(len(target_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "a6127f19-7f99-4389-88a9-ab71b2849165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "8e53e39d-1617-4ea6-a6cf-ff16f08bd011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée les dictionnaires des 2 vocabulaires\n",
    "cnt = Counter()\n",
    "for sentence in x_train:\n",
    "    for word in sentence:\n",
    "        cnt[word] += 1\n",
    "\n",
    "li = cnt.most_common(len(vocab))\n",
    "vocab = {}\n",
    "for i in range(len(li)):\n",
    "    word, n = li[i]\n",
    "    vocab[word] = i + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "35ff652d-0583-48bc-8aca-9e6d5efc416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter()\n",
    "for sentence in y_train:\n",
    "    for word in sentence:\n",
    "        cnt[word] += 1\n",
    "\n",
    "li = cnt.most_common(len(target_vocab))\n",
    "target_vocab = {}\n",
    "for i in range(len(li)):\n",
    "    word, n = li[i]\n",
    "    target_vocab[word] = i + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "56b67544-1049-4bc2-999e-6eb3c2da6c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On enlève les mots qui n'appartiennent pas au vocab et remplace les autre par un entier\n",
    "for sentence in x_train:\n",
    "    for i in range(len(sentence) - 1, -1, -1):\n",
    "        if sentence[i] not in vocab:\n",
    "            sentence.pop(i)\n",
    "        else:\n",
    "            sentence[i] = vocab[sentence[i]]\n",
    "\n",
    "for sentence in x_test:\n",
    "    for i in range(len(sentence) - 1, -1, -1):\n",
    "        if sentence[i] not in vocab:\n",
    "            sentence.pop(i)\n",
    "        else:\n",
    "            sentence[i] = vocab[sentence[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "79491abf-1b39-493a-a1bd-1708b2665a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in y_train:\n",
    "    for i in range(len(sentence) - 1, -1, -1):\n",
    "        if sentence[i] not in target_vocab:\n",
    "            sentence.pop(i)\n",
    "        else:\n",
    "            sentence[i] = target_vocab[sentence[i]]\n",
    "\n",
    "for sentence in y_test:\n",
    "    for i in range(len(sentence) - 1, -1, -1):\n",
    "        if sentence[i] not in target_vocab:\n",
    "            sentence.pop(i)\n",
    "        else:\n",
    "            sentence[i] = target_vocab[sentence[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "d4dd3817-5a91-4f9b-9387-5c099ac2f7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permet de transformer chaque phrase en un vecteur de dim max_length + 2\n",
    "def make_vector(li, max_length):\n",
    "    if len(li) > max_length:\n",
    "        return [1] + li[:max_length] + [2]\n",
    "    else:\n",
    "        return [1] + li + [3 for i in range(max_length - len(li))] + [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "5ff529f5-ec3e-4dc8-bf7a-110f56df6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On applique la fonction a nos phrases\n",
    "for i in range(len(x_train)):\n",
    "    x_train[i] = make_vector(x_train[i], seq_len)\n",
    "for i in range(len(y_train)):\n",
    "    y_train[i] = make_vector(y_train[i], seq_len)\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    x_test[i] = make_vector(x_test[i], seq_len)\n",
    "for i in range(len(y_test)):\n",
    "    y_test[i] = make_vector(y_test[i], seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "87e63a0a-16b2-4972-8782-ae8582734feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch import IntTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "9bd0dc57-d004-4a4c-acc3-481d671dd874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des Dataloader\n",
    "trainset = TensorDataset(IntTensor(x_train), IntTensor(y_train))\n",
    "train_dataloader = DataLoader(trainset, batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "testset = TensorDataset(IntTensor(x_test), IntTensor(y_test))\n",
    "test_dataloader = DataLoader(testset, batch_size=1, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59756bcd-fe3f-4687-a17b-2324a95cd264",
   "metadata": {},
   "source": [
    "# Entrainement du transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "2aa7785f-f85e-4581-8070-50327963b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "b4851d60-806b-4f18-b1d4-46dc0769638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des hyperparamètres\n",
    "tf1 = Transformer(len(vocab) + 4,len(target_vocab) + 4, seq_len + 2, 512, 8, 1024, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "1ce976a0-68a6-40a7-a6fe-5d2709fa2fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(tf1.parameters(),lr=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "a7e5bc81-6d05-47a8-8d60-47cbaa43ef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        batch_size = len(y)\n",
    "        pred = model(X, y)\n",
    "        pred = pred.view(-1, pred.size(-1))  \n",
    "        y = y.view(-1)  \n",
    "        y = y.long()\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "7bb21a0f-f080-48b7-95d9-01c027f6517d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 7.963542  [   32/32000]\n",
      "loss: 7.371623  [ 3232/32000]\n",
      "loss: 6.790514  [ 6432/32000]\n",
      "loss: 6.258421  [ 9632/32000]\n",
      "loss: 5.786921  [12832/32000]\n",
      "loss: 5.276005  [16032/32000]\n",
      "loss: 4.723555  [19232/32000]\n",
      "loss: 4.324573  [22432/32000]\n",
      "loss: 3.934635  [25632/32000]\n",
      "loss: 3.510591  [28832/32000]\n",
      "loss: 3.081415  [   32/32000]\n",
      "loss: 2.749336  [ 3232/32000]\n",
      "loss: 2.539872  [ 6432/32000]\n",
      "loss: 2.242742  [ 9632/32000]\n",
      "loss: 2.076568  [12832/32000]\n",
      "loss: 1.973610  [16032/32000]\n",
      "loss: 1.909057  [19232/32000]\n",
      "loss: 1.760130  [22432/32000]\n",
      "loss: 1.733293  [25632/32000]\n",
      "loss: 1.685493  [28832/32000]\n",
      "loss: 1.626862  [   32/32000]\n",
      "loss: 1.587809  [ 3232/32000]\n",
      "loss: 1.470157  [ 6432/32000]\n",
      "loss: 1.462643  [ 9632/32000]\n",
      "loss: 1.457907  [12832/32000]\n",
      "loss: 1.531557  [16032/32000]\n",
      "loss: 1.334028  [19232/32000]\n",
      "loss: 1.384270  [22432/32000]\n",
      "loss: 1.323455  [25632/32000]\n",
      "loss: 1.300388  [28832/32000]\n"
     ]
    }
   ],
   "source": [
    "# Entrainement sur 3 epochs\n",
    "nb_epoch = 3\n",
    "for i in range(nb_epoch):\n",
    "    train_loop(train_dataloader, tf1,loss_fn, adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7e911d-fff1-465b-99e5-ba29ca44c14a",
   "metadata": {},
   "source": [
    "# Prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "3b1ee318-72e1-42c8-bd8c-315d2f0379b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src_tensor, max_len=seq_len + 2, start_token=1, end_token=2):\n",
    "    output = []\n",
    "\n",
    "    batch_size = src_tensor.shape[0]  \n",
    "    dec_input = torch.full((1, batch_size), start_token, dtype=torch.long)  \n",
    "\n",
    "    # On transpose pour respecter l'ordre des dimensions\n",
    "    src_tensor = src_tensor.transpose(0, 1)  \n",
    "    \n",
    "    for i in range(max_len):\n",
    "        # On recupère l'output du modèle\n",
    "        dec_output = model(src_tensor, dec_input) \n",
    "\n",
    "        # On sélectionne le token avec la meilleur probabilité\n",
    "        next_token = dec_output[-1, :, :].argmax(dim=-1)  \n",
    "        \n",
    "        output.extend(next_token.tolist())  \n",
    "        \n",
    "        # On le convertit pour pouvoir l'ajouter à l'input du decodeur\n",
    "        next_token = next_token.unsqueeze(0)  \n",
    "        next_token = next_token.transpose(0,1)\n",
    "        \n",
    "        # On concatene le token avec l'input du décodeur\n",
    "        dec_input = torch.cat([dec_input, next_token], dim=0)  \n",
    "\n",
    "\n",
    "        # Si le token de fin est généré on s'arrete\n",
    "        if (next_token == end_token).all():  \n",
    "            break\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "336896ff-673d-4029-8877-7005fd9c00ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforme une liste de token en string\n",
    "def decode(x):\n",
    "    for i in range(len(x)):\n",
    "        if x[i] <= 3:\n",
    "            x[i] = \"\"\n",
    "        else:    \n",
    "            for keys in target_vocab.keys():\n",
    "                if x[i] == target_vocab[keys]:\n",
    "                    x[i] = keys + \" \"\n",
    "    sentence = \"\"\n",
    "    for i in range(len(x)):\n",
    "        sentence += x[i] \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "33962b3b-f4d2-48f5-bc9b-171ddf4cafaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "a940c828-b351-4b55-a570-0563316fb293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donne la moyenne du score BLEU du model sur les données d'un dataloader\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    total_bleu = 0\n",
    "    for  batch, (X, y) in enumerate(dataloader): \n",
    "        predicted_tokens = greedy_decode(model, X)\n",
    "        predicted_translation = decode(predicted_tokens)\n",
    "        \n",
    "        target = decode(y.tolist()[0])\n",
    "        bleu_score = sentence_bleu(target,predicted_translation )\n",
    "        total_bleu += bleu_score\n",
    "\n",
    "    avg_bleu = total_bleu / len(dataloader)\n",
    "    print(f\"Average BLEU Score: {avg_bleu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "ddf990b0-fe76-4fcc-a7c8-c84e60f4bd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(tf1, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "ad6867c4-1511-4447-ab7c-be45417e6017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prend un string et renvoie le string qui est donnée par le model\n",
    "def translate(x,model = tf1):\n",
    "    x = word_tokenize(x)\n",
    "    for i in range(len(x) - 1, -1, -1):\n",
    "        if x[i] in punctuation:\n",
    "            x.pop(i)\n",
    "    x = [stemmer.stem(word) for word in x]\n",
    "    for i in range(len(x) - 1, -1, -1):\n",
    "        if x[i] not in vocab:\n",
    "            x.pop(i)\n",
    "        else:\n",
    "            x[i] = vocab[x[i]]\n",
    "    x = make_vector(x, seq_len)\n",
    "    x = [x]\n",
    "    xset = TensorDataset(IntTensor(x), IntTensor(x))\n",
    "    x_dataloader = DataLoader(xset, batch_size=1, shuffle=True, drop_last=True)\n",
    "    \n",
    "    for  batch, (X, y) in enumerate(x_dataloader):\n",
    "        predicted_tokens = greedy_decode(model, X)\n",
    "        print(predicted_tokens)\n",
    "        print(batch)\n",
    "        x = decode(predicted_tokens)   \n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "a643db19-e257-42a6-ad5e-9a3ae41f04ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Etre ou ne pas etre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adee14f-3a26-4b69-abcb-85a266893262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
