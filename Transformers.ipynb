{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7fcd268-afaa-4537-9a25-b31015ac3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b28d6ac-6587-4372-a7e6-30ccc193aa29",
   "metadata": {},
   "source": [
    "# Création de la classe Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "11ca8946-14c1-412b-8ceb-bbcc9b7891bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_nans(tensor, tensor_name):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"NaN detected in {tensor_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f4c4f7-aeae-4eb8-9e6a-425adc80d048",
   "metadata": {},
   "source": [
    "Création des différentes couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "0b435002-af4e-4743-8939-31101cb2422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "38f8d9a1-8aec-4970-9a73-141c74d8d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionel : Coder notre propre couche multi-attention head (pour ne pas utiliser celle fournit par torch.nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "0636a980-6485-4023-b485-732845a49f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(x):\n",
    "    len = x.size(0)\n",
    "    mask = torch.triu(torch.ones(len,len), diagonal = 1) * (-1e9) # Matrice triangulaire supérieur de valeur -inf\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "7f89e55d-5a9d-4186-98c1-3f4d042fc7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, max_length, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "1b1a857c-a787-4f5b-8408-093e8a636158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.attention = torch.nn.MultiheadAttention(d_model, n_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_output, wei = self.attention(x, x, x)\n",
    "        x = self.norm1(x + attention_output)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "2df9652d-bc17-4923-a110-72fb57893c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.attention1 = torch.nn.MultiheadAttention(d_model, n_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attention2 = torch.nn.MultiheadAttention(d_model, n_heads)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, enc_output, mask):\n",
    "        attention_output1, wei = self.attention1(x, x, x,attn_mask = mask)\n",
    "        x = self.norm1(x + attention_output1)\n",
    "        attention_output2, wei = self.attention2(x, enc_output, enc_output)\n",
    "        x = self.norm2(x + attention_output2)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm3(x + ffn_output)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "2f440a5e-05d8-48ad-97c3-8b42eeaa1e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, target_size, max_length, d_model, num_heads, d_ff, n_layers):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.enc_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.dec_embedding = nn.Embedding(target_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(max_length, d_model)\n",
    "        \n",
    "        self.encoder_layers = [Encoder(d_model, num_heads,d_ff) for i in range(n_layers)]\n",
    "        self.decoder_layers = [Decoder(d_model, num_heads, d_ff) for i in range(n_layers)]\n",
    "\n",
    "        self.linear = nn.Linear(d_model, target_size)\n",
    "\n",
    "    def check_for_nans(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if torch.isnan(param).any():\n",
    "                print(f\"NaN detected in parameter: {name}\")\n",
    "\n",
    "    def forward(self,inp,out):\n",
    "\n",
    "        check_for_nans(inp,\"inp\")\n",
    "        check_for_nans(out, \"out\")\n",
    "        \n",
    "        mask = create_mask(out)\n",
    "\n",
    "        check_for_nans(mask, \"mask\")\n",
    "        \n",
    "        out_embedded = self.positional_encoding(self.dec_embedding(out))\n",
    "        inp_embedded = self.positional_encoding(self.enc_embedding(inp))\n",
    "\n",
    "        check_for_nans(out_embedded, \"out_embedded\")\n",
    "        check_for_nans(inp_embedded, \"inp_embedded\")\n",
    "        \n",
    "        enc_output = inp_embedded\n",
    "        for encoder in self.encoder_layers:\n",
    "            enc_output = encoder(enc_output)\n",
    "\n",
    "        check_for_nans(enc_output, \"enc_output\")\n",
    "        \n",
    "        dec_output = out_embedded\n",
    "        for decoder in self.decoder_layers:\n",
    "            dec_output = decoder(dec_output, enc_output, mask)\n",
    "\n",
    "        check_for_nans(dec_output, \"dec_output\")\n",
    "        \n",
    "        output = self.linear(dec_output)\n",
    "\n",
    "        check_for_nans(output, \"output\")\n",
    "        return output   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bca86b-2ff1-4662-b9c4-8eb454e41c00",
   "metadata": {},
   "source": [
    "# Mise en forme des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "de786ef9-1eff-42d1-8194-4089c3bfb59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "18c64f66-c4ab-4f7b-b0c6-dab3315ece99",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "with open(\"eng_-french.csv\",encoding = \"utf-8\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        y.append(row[0])\n",
    "        x.append(row[-1])\n",
    "    x.pop(0)\n",
    "    y.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "967b8ebd-00d2-416e-9912-8aa2e7a9bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_red = x[:10000]\n",
    "y_red = y[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "a43238cc-b560-43c9-8b7e-54c966e14c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "9f7bdd50-a7f2-46e1-b458-61c837afead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_red, y_red, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "91a6d0a8-833c-4907-9df4-3829664415b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [word_tokenize(word) for word in x_train]\n",
    "y_train = [word_tokenize(word) for word in y_train]\n",
    "x_test = [word_tokenize(word) for word in x_test]\n",
    "y_test = [word_tokenize(word) for word in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "id": "f8082190-7af8-422d-b9e3-e9555a376944",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_train)):\n",
    "    for j in range(len(x_train[i])-1,-1,-1):\n",
    "        if x_train[i][j] in punctuation:\n",
    "            x_train[i].pop(j)\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    for j in range(len(y_train[i])-1,-1,-1):\n",
    "        if y_train[i][j] in punctuation:\n",
    "            y_train[i].pop(j)\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    for j in range(len(x_test[i])-1,-1,-1):\n",
    "        if x_test[i][j] in punctuation:\n",
    "            x_test[i].pop(j)\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    for j in range(len(y_test[i])-1,-1,-1):\n",
    "        if y_test[i][j] in punctuation:\n",
    "            y_test[i].pop(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "742b3078-e24b-4aa9-92e6-9151cf6b2a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "for i in range(len(y_train)):\n",
    "    y_train[i] = [stemmer.stem(word) for word in y_train[i]]\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    x_train[i] = [stemmer.stem(word) for word in x_train[i]]\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    y_test[i] = [stemmer.stem(word) for word in y_test[i]]\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    x_test[i] = [stemmer.stem(word) for word in x_test[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "ab48ef8b-7698-40b7-872e-bbea32e9a233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3394\n",
      "1618\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "target_vocab = []\n",
    "for sentence in x_train:\n",
    "    for word in sentence:\n",
    "        if word not in vocab:\n",
    "            vocab.append(word)\n",
    "\n",
    "for sentence in y_train:\n",
    "    for word in sentence:\n",
    "        if word not in target_vocab:\n",
    "            target_vocab.append(word)\n",
    "\n",
    "print(len(vocab))\n",
    "print(len(target_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "a6127f19-7f99-4389-88a9-ab71b2849165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "8e53e39d-1617-4ea6-a6cf-ff16f08bd011",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter()\n",
    "for sentence in x_train:\n",
    "    for word in sentence:\n",
    "        cnt[word] += 1\n",
    "\n",
    "li = cnt.most_common(len(vocab))\n",
    "vocab = {}\n",
    "for i in range(len(li)):\n",
    "    word, n = li[i]\n",
    "    vocab[word] = i + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "35ff652d-0583-48bc-8aca-9e6d5efc416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter()\n",
    "for sentence in y_train:\n",
    "    for word in sentence:\n",
    "        cnt[word] += 1\n",
    "\n",
    "li = cnt.most_common(len(target_vocab))\n",
    "target_vocab = {}\n",
    "for i in range(len(li)):\n",
    "    word, n = li[i]\n",
    "    target_vocab[word] = i + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "56b67544-1049-4bc2-999e-6eb3c2da6c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in x_train:\n",
    "    for i in range(len(sentence) - 1, -1, -1):\n",
    "        if sentence[i] not in vocab:\n",
    "            sentence.pop(i)\n",
    "        else:\n",
    "            sentence[i] = vocab[sentence[i]]\n",
    "\n",
    "for sentence in x_test:\n",
    "    for i in range(len(sentence) - 1, -1, -1):\n",
    "        if sentence[i] not in vocab:\n",
    "            sentence.pop(i)\n",
    "        else:\n",
    "            sentence[i] = vocab[sentence[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "79491abf-1b39-493a-a1bd-1708b2665a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in y_train:\n",
    "    for i in range(len(sentence) - 1, -1, -1):\n",
    "        if sentence[i] not in target_vocab:\n",
    "            sentence.pop(i)\n",
    "        else:\n",
    "            sentence[i] = target_vocab[sentence[i]]\n",
    "\n",
    "for sentence in y_test:\n",
    "    for i in range(len(sentence) - 1, -1, -1):\n",
    "        if sentence[i] not in target_vocab:\n",
    "            sentence.pop(i)\n",
    "        else:\n",
    "            sentence[i] = target_vocab[sentence[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "d4dd3817-5a91-4f9b-9387-5c099ac2f7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vector(li, max_length):\n",
    "    if len(li) > max_length:\n",
    "        return [1] + li[:max_length] + [2]\n",
    "    else:\n",
    "        return [1] + li + [3 for i in range(max_length - len(li))] + [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "5ff529f5-ec3e-4dc8-bf7a-110f56df6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_train)):\n",
    "    x_train[i] = make_vector(x_train[i], 40)\n",
    "for i in range(len(y_train)):\n",
    "    y_train[i] = make_vector(y_train[i], 40)\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    x_test[i] = make_vector(x_test[i], 40)\n",
    "for i in range(len(y_test)):\n",
    "    y_test[i] = make_vector(y_test[i], 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "87e63a0a-16b2-4972-8782-ae8582734feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch import IntTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "9bd0dc57-d004-4a4c-acc3-481d671dd874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des Dataloader\n",
    "trainset = TensorDataset(IntTensor(x_train), IntTensor(y_train))\n",
    "train_dataloader = DataLoader(trainset, batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "testset = TensorDataset(IntTensor(x_test), IntTensor(y_test))\n",
    "test_dataloader = DataLoader(testset, batch_size=32, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59756bcd-fe3f-4687-a17b-2324a95cd264",
   "metadata": {},
   "source": [
    "# Entrainement du transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "2aa7785f-f85e-4581-8070-50327963b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "b4851d60-806b-4f18-b1d4-46dc0769638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1 = Transformer(len(vocab) + 4,len(target_vocab) + 4, 42, 512, 8, 1024, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "id": "1ce976a0-68a6-40a7-a6fe-5d2709fa2fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(tf1.parameters(),lr=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "id": "a7e5bc81-6d05-47a8-8d60-47cbaa43ef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        batch_size = len(y)\n",
    "        pred = model(X, y)\n",
    "        pred = pred.view(-1, pred.size(-1))  # [batch_size * seq_len, num_classes]\n",
    "        y = y.view(-1)  # [batch_size * seq_len]\n",
    "        y = y.long()\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "id": "7bb21a0f-f080-48b7-95d9-01c027f6517d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 7.233064  [   32/ 8000]\n",
      "loss: 6.532442  [ 3232/ 8000]\n",
      "loss: 5.822691  [ 6432/ 8000]\n",
      "loss: 5.472999  [   32/ 8000]\n",
      "loss: 4.760701  [ 3232/ 8000]\n",
      "loss: 4.082810  [ 6432/ 8000]\n",
      "loss: 3.751346  [   32/ 8000]\n",
      "loss: 3.085642  [ 3232/ 8000]\n",
      "loss: 2.524326  [ 6432/ 8000]\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 3\n",
    "for i in range(nb_epoch):\n",
    "    train_loop(train_dataloader, tf1,loss_fn, adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7e911d-fff1-465b-99e5-ba29ca44c14a",
   "metadata": {},
   "source": [
    "# Prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "id": "3b1ee318-72e1-42c8-bd8c-315d2f0379b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src_tensor, max_len=50, start_token=1, end_token=2):\n",
    "    output = []\n",
    "    # Initial decoder token (e.g., start token)\n",
    "    dec_input = torch.tensor([start_token], dtype=torch.long).unsqueeze(0)  # Shape: (1, 1)\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        # Get model output: (batch_size, seq_len, vocab_size)\n",
    "        dec_output = model(src_tensor, dec_input)\n",
    "        \n",
    "        # Select the most likely token at each position (take the max over vocab_size dimension)\n",
    "        # dec_output is (batch_size, seq_len, vocab_size), we want to pick the most probable token\n",
    "        # So we pick the token with the max logit for each position\n",
    "        next_token = dec_output[:, -1, :].argmax(dim=-1)  # Shape: (batch_size,)\n",
    "        \n",
    "        output.append(next_token.item())  # Convert tensor to scalar (single token)\n",
    "        \n",
    "        # Update the decoder input for the next step (the next token will be the last generated token)\n",
    "        dec_input = next_token.unsqueeze(0).long  # Shape: (1, 1)\n",
    "        print(i)\n",
    "        if next_token.item() == end_token:  # Stop if end token is generated\n",
    "            break\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "id": "336896ff-673d-4029-8877-7005fd9c00ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(x):\n",
    "    for numb in x:\n",
    "        for keys in target_vocab.keys():\n",
    "            if numb == target_vocab[keys]:\n",
    "                numb = key\n",
    "    return x     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "id": "33962b3b-f4d2-48f5-bc9b-171ddf4cafaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "id": "a940c828-b351-4b55-a570-0563316fb293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    total_bleu = 0\n",
    "    for x, y in dataloader:\n",
    "\n",
    "        predicted_tokens = greedy_decode(model, x)\n",
    "        predicted_translation = decode(predicted_tokens)\n",
    "        \n",
    "        bleu_score = sentence_bleu([decode(y)], predicted_translation)\n",
    "        total_bleu += bleu_score\n",
    "\n",
    "    avg_bleu = total_bleu / len(dataloader)\n",
    "    print(f\"Average BLEU Score: {avg_bleu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "id": "ddf990b0-fe76-4fcc-a7c8-c84e60f4bd55",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32, 8, 64]' is invalid for input of size 688128",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[760], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m evaluate_model(tf1, test_dataloader)\n",
      "Cell \u001b[1;32mIn[758], line 6\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, dataloader)\u001b[0m\n\u001b[0;32m      3\u001b[0m total_bleu \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m----> 6\u001b[0m     predicted_tokens \u001b[38;5;241m=\u001b[39m greedy_decode(model, x)\n\u001b[0;32m      7\u001b[0m     predicted_translation \u001b[38;5;241m=\u001b[39m decode(predicted_tokens)\n\u001b[0;32m      9\u001b[0m     bleu_score \u001b[38;5;241m=\u001b[39m sentence_bleu([decode(y)], predicted_translation)\n",
      "Cell \u001b[1;32mIn[752], line 8\u001b[0m, in \u001b[0;36mgreedy_decode\u001b[1;34m(model, src_tensor, max_len, start_token, end_token)\u001b[0m\n\u001b[0;32m      4\u001b[0m dec_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([start_token], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Shape: (1, 1)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_len):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Get model output: (batch_size, seq_len, vocab_size)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     dec_output \u001b[38;5;241m=\u001b[39m model(src_tensor, dec_input)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Select the most likely token at each position (take the max over vocab_size dimension)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# dec_output is (batch_size, seq_len, vocab_size), we want to pick the most probable token\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# So we pick the token with the max logit for each position\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     next_token \u001b[38;5;241m=\u001b[39m dec_output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape: (batch_size,)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[616], line 43\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, inp, out)\u001b[0m\n\u001b[0;32m     41\u001b[0m dec_output \u001b[38;5;241m=\u001b[39m out_embedded\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers:\n\u001b[1;32m---> 43\u001b[0m     dec_output \u001b[38;5;241m=\u001b[39m decoder(dec_output, enc_output, mask)\n\u001b[0;32m     45\u001b[0m check_for_nans(dec_output, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdec_output\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(dec_output)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[614], line 16\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, x, enc_output, mask)\u001b[0m\n\u001b[0;32m     14\u001b[0m attention_output1, wei \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention1(x, x, x,attn_mask \u001b[38;5;241m=\u001b[39m mask)\n\u001b[0;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m attention_output1)\n\u001b[1;32m---> 16\u001b[0m attention_output2, wei \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention2(x, enc_output, enc_output)\n\u001b[0;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m attention_output2)\n\u001b[0;32m     18\u001b[0m ffn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(x)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1266\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1252\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1253\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1254\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1263\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1264\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m   1265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1266\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1267\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1268\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   1269\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_v, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_zero_attn,\n\u001b[0;32m   1270\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m   1271\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[0;32m   1272\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[0;32m   1273\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39mneed_weights,\n\u001b[0;32m   1274\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[0;32m   1275\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1276\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:5410\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   5408\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mview(tgt_len, bsz \u001b[38;5;241m*\u001b[39m num_heads, head_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   5409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m static_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 5410\u001b[0m     k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mview(k\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], bsz \u001b[38;5;241m*\u001b[39m num_heads, head_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   5411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5412\u001b[0m     \u001b[38;5;66;03m# TODO finish disentangling control flow so we don't do in-projections when statics are passed\u001b[39;00m\n\u001b[0;32m   5413\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m static_k\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m bsz \u001b[38;5;241m*\u001b[39m num_heads, \\\n\u001b[0;32m   5414\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting static_k.size(0) of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbsz\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mnum_heads\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatic_k\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[32, 8, 64]' is invalid for input of size 688128"
     ]
    }
   ],
   "source": [
    "evaluate_model(tf1, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37783ad-1db0-447b-8500-fa2a2ea9e51f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
